{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "\n",
    "# --- setup ---\n",
    "d = 100\n",
    "\n",
    "base_lr = 1e-1\n",
    "x = (torch.pi * (2 * torch.rand(d) - 1)).clone().detach().requires_grad_(True)  # leaf tensor\n",
    "\n",
    "optimizer = optim.SGD([x], lr=base_lr, momentum=0.9)     # swap in RMSprop, Adadelta, etc.\n",
    "# optimizer = optim.RMSprop([x], lr=base_lr, momentum=0.9)     # swap in RMSprop, Adadelta, etc.\n",
    "# optimizer = Lion([x], lr=base_lr, weight_decay=1e-3)     # swap in RMSprop, Adadelta, etc.\n",
    "\n",
    "def measure_fn(x_vec):\n",
    "    \"\"\"Simulated experiment: returns metric to minimize.\"\"\"\n",
    "    phi_noise = 2 * torch.pi / 100 * torch.randn(d)\n",
    "    value = torch.abs(torch.sum(torch.exp(1j * (x_vec + phi_noise))))\n",
    "    best_value = torch.abs(torch.sum(torch.exp(1j * torch.zeros(d))))\n",
    "    return 1 - value / best_value # normalize to [0,1]\n",
    "\n",
    "# --- logging ---\n",
    "metrics = []\n",
    "trajectories = []\n",
    "lrs = []\n",
    "prev_metric = None\n",
    "m_perturbs = 1\n",
    "\n",
    "# --- online loop ---\n",
    "for k in range(5000):\n",
    "    # SPSA perturbation\n",
    "    c = 0.1\n",
    "    \n",
    "    # Averaging gradient estimate over m_perturbs\n",
    "    ghat = torch.zeros_like(x)\n",
    "    for _ in range(m_perturbs):\n",
    "        Delta = torch.randint(0, 2, x.shape) * 2 - 1\n",
    "        xp = x.detach() + c * Delta\n",
    "        xm = x.detach() - c * Delta\n",
    "        y_plus = measure_fn(xp)\n",
    "        y_minus = measure_fn(xm)\n",
    "        ghat += ((y_plus - y_minus) / (2 * c)) * Delta\n",
    "    ghat /= m_perturbs\n",
    "\n",
    "    # Apply optimizer update\n",
    "    optimizer.zero_grad()\n",
    "    x.grad = ghat.clone()\n",
    "    # x = torch.angle(torch.exp(1j * (x - x.mean())))  # wrap angles to [-pi,+pi]\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # \n",
    "\n",
    "    # --- enforce bounds [0,1] ---\n",
    "    # with torch.no_grad():\n",
    "    #     x.clamp_(-torch.pi, +torch.pi)\n",
    "\n",
    "    # Track metric and trajectory\n",
    "    metric_now = measure_fn(x.detach())\n",
    "    metrics.append(metric_now.item())\n",
    "    trajectories.append(x.detach().clone().numpy())\n",
    "\n",
    "    # --- adaptive learning rate ---\n",
    "    if prev_metric is not None:\n",
    "        if metric_now < prev_metric:   # improved\n",
    "            for g in optimizer.param_groups:\n",
    "                g[\"lr\"] *= 1.0      # increase LR by 5%\n",
    "        else:                          # worse\n",
    "            for g in optimizer.param_groups:\n",
    "                g[\"lr\"] *= 01.0       # decrease LR by 30%\n",
    "    prev_metric = metric_now\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "# --- convert logs ---\n",
    "trajectories = torch.tensor(trajectories)  # shape (steps, d)\n",
    "\n",
    "# --- plots ---\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot metric convergence\n",
    "axs[0].plot(metrics, lw=2)\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"Metric (to minimize)\")\n",
    "axs[0].set_title(f\"Convergence of SPSA + {optimizer.__class__.__name__}\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].set_ylim(1e-3, 1)\n",
    "\n",
    "# Plot first 3 parameters trajectories\n",
    "axs[1].plot(trajectories, label=\"x[0]\")\n",
    "# axs[1].plot(trajectories[:, 1], label=\"x[1]\")\n",
    "# axs[1].plot(trajectories[:, 2], label=\"x[2]\")\n",
    "axs[1].axhline(0.5, color=\"k\", ls=\"--\", label=\"target\")\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(\"Parameter value\")\n",
    "axs[1].set_title(\"Parameter trajectories\")\n",
    "# axs[1].legend()\n",
    "\n",
    "axs[2].plot(lrs, lw=2)\n",
    "axs[2].set_xlabel(\"Iteration\")\n",
    "axs[2].set_ylabel(\"Learning rate\")\n",
    "axs[2].set_title(\"Adaptive learning rate evolution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
